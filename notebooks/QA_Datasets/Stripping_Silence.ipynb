{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stripping Silence\n",
    "\n",
    "We use this notebook to compare different techniques for cutting silences on the LJ speech dataset.\n",
    "\n",
    "One of the problems with the Linda John speech dataset, is the the dataset is cut between sentences; therefore, there can be unpredictable silence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import random\n",
    "\n",
    "# Setup the \"PYTHONPATH\"\n",
    "sys.path.insert(0, '../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Audio, Markdown\n",
    "\n",
    "class UnnormalizedAudio(Audio):\n",
    "\n",
    "    def _make_wav(self, data, rate):\n",
    "        \"\"\" Transform a numpy array to a PCM bytestring \"\"\"\n",
    "        import struct\n",
    "        from io import BytesIO\n",
    "        import wave\n",
    "\n",
    "        try:\n",
    "            import numpy as np\n",
    "\n",
    "            data = np.array(data, dtype=float)\n",
    "            if len(data.shape) == 1:\n",
    "                nchan = 1\n",
    "            elif len(data.shape) == 2:\n",
    "                # In wave files,channels are interleaved. E.g.,\n",
    "                # \"L1R1L2R2...\" for stereo. See\n",
    "                # http://msdn.microsoft.com/en-us/library/windows/hardware/dn653308(v=vs.85).aspx\n",
    "                # for channel ordering\n",
    "                nchan = data.shape[0]\n",
    "                data = data.T.ravel()\n",
    "            else:\n",
    "                raise ValueError('Array audio input must be a 1D or 2D array')\n",
    "            scaled = np.int16(data*32767).tolist()\n",
    "        except ImportError:\n",
    "            # check that it is a \"1D\" list\n",
    "            idata = iter(data)  # fails if not an iterable\n",
    "            try:\n",
    "                iter(idata.next())\n",
    "                raise TypeError('Only lists of mono audio are '\n",
    "                    'supported if numpy is not installed')\n",
    "            except TypeError:\n",
    "                # this means it's not a nested list, which is what we want\n",
    "                pass\n",
    "            scaled = [int(x*32767) for x in data]\n",
    "            nchan = 1\n",
    "\n",
    "        fp = BytesIO()\n",
    "        waveobj = wave.open(fp,mode='wb')\n",
    "        waveobj.setnchannels(nchan)\n",
    "        waveobj.setframerate(rate)\n",
    "        waveobj.setsampwidth(2)\n",
    "        waveobj.setcomptype('NONE','NONE')\n",
    "        waveobj.writeframes(b''.join([struct.pack('<h',x) for x in scaled]))\n",
    "        val = fp.getvalue()\n",
    "        waveobj.close()\n",
    "\n",
    "        return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import hparams\n",
    "\n",
    "hparams.set_hparams()\n",
    "train, dev = hparams.get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot\n",
    "\n",
    "def plot_waveform(signals, labels=None):\n",
    "    \"\"\" Plot a wave form\n",
    "    \n",
    "    Args:\n",
    "        signals (list): List of signals to plot.\n",
    "        labels (list of str, optional): Labels to add to signals.\n",
    "    \"\"\"\n",
    "    pyplot.figure(figsize=(20,5))\n",
    "    for i, signal in enumerate(signals):\n",
    "        label = None if labels is None else labels[i]\n",
    "        pyplot.plot(signal, label=label)\n",
    "    if labels is not None:\n",
    "        pyplot.legend()\n",
    "    pyplot.ylim(-1, 1)\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Audio, Markdown\n",
    "from tqdm import tqdm\n",
    "\n",
    "import IPython\n",
    "import numpy as np\n",
    "\n",
    "from src.audio import read_audio\n",
    "\n",
    "\n",
    "def evaluate_trim(trim, max_rows=1000, max_samples=5000, top_k=5):\n",
    "    \"\"\" Evaluate a silence trimming algorithm.\n",
    "    \n",
    "    Args:\n",
    "        max_rows (int): Maximum rows to evaluate.\n",
    "        max_samples (int): Maximum samples to display at the end and beginning of the signal.\n",
    "        top_k (int): Top k signals to display.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for row in tqdm(train[:max_rows]):\n",
    "        signal, sample_rate = librosa.core.load(str(row.audio_path), sr=24000)\n",
    "        trimmed = trim(str(row.audio_path), signal)\n",
    "        length_difference = len(signal) - len(trimmed)\n",
    "        if length_difference > 0:\n",
    "            energy_difference =  (np.sum(np.absolute(signal)) - np.sum(np.absolute(trimmed))) / length_difference\n",
    "            results.append({'signal': np.array(signal),\n",
    "                            'trimmed': np.array(trimmed),\n",
    "                            'energy_difference': energy_difference,\n",
    "                            'length_difference': length_difference,\n",
    "                            'filename': row.audio_path,\n",
    "                            'text': row.text})\n",
    "\n",
    "    results = sorted(results, key=lambda r: r['energy_difference'], reverse=True)\n",
    "    display(Markdown('### Top %d Signals Affected by Trim' % top_k))\n",
    "    for result in (results[:top_k]):\n",
    "        display(Markdown('File: %s' % (result['filename'],)))\n",
    "        display(Markdown('Text: %s' % (result['text'],)))\n",
    "        display(Markdown('Energy Difference: %f' % (result['energy_difference'],)))\n",
    "        display(Markdown('Length Difference: %d' % (result['length_difference'],)))\n",
    "        \n",
    "        display(Markdown('Trimmed:'))\n",
    "        display(UnnormalizedAudio(result['trimmed'], rate=sample_rate))\n",
    "        plot_waveform([result['trimmed']])\n",
    "        \n",
    "        display(Markdown('Signal:'))\n",
    "        display(UnnormalizedAudio(result['signal'], rate=sample_rate))\n",
    "        plot_waveform([result['signal']])\n",
    "            \n",
    "        display(Markdown('---'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import librosa\n",
    "\n",
    "trim = lambda _, s: librosa.effects.trim(s, frame_length=1024, hop_length=256)[0]\n",
    "evaluate_trim(trim, top_k=20, max_rows=sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyDub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydub\n",
    "from pydub import AudioSegment\n",
    "\n",
    "def pydub_trim(wav, _):\n",
    "    sound = AudioSegment.from_wav(wav)\n",
    "    pydub.effects.strip_silence(sound)\n",
    "    return sound.get_array_of_samples()\n",
    "\n",
    "evaluate_trim(pydub_trim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOX\n",
    "\n",
    "Reference:\n",
    "https://digitalcardboard.com/blog/2009/08/25/the-sox-of-silence/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "destination = os.path.abspath('temp.wav')\n",
    "    \n",
    "def sox_trim(wav, _):\n",
    "    os.system('sox {} {} silence 1 0.01 1% reverse silence 1 0.01 1% reverse'.format(wav, destination))\n",
    "    trimmed = read_audio(destination)\n",
    "    return trimmed\n",
    "\n",
    "evaluate_trim(sox_trim)\n",
    "\n",
    "# Clean up\n",
    "os.remove(destination)\n",
    "assert not os.path.isfile(destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ibab/tensorflow-wavenet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_silence(_, audio, threshold=0.1, frame_length=2048):\n",
    "    '''Removes silence at the beginning and end of a sample.'''\n",
    "    audio = np.array(audio, dtype=np.float)\n",
    "    if audio.size < frame_length:\n",
    "        frame_length = audio.size\n",
    "    energy = librosa.feature.rmse(audio, frame_length=frame_length)\n",
    "    frames = np.nonzero(energy > threshold)\n",
    "    indices = librosa.core.frames_to_samples(frames)[1]\n",
    "\n",
    "    # Note: indices can be an empty array, if the whole audio was silence.\n",
    "    return audio[indices[0]:indices[-1]] if indices.size else audio[0:0]\n",
    "\n",
    "evaluate_trim(trim_silence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
