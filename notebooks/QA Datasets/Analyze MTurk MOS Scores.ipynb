{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze MTurk MOS Scores\n",
    "\n",
    "This repository analyzes MOS scores from MTurk.\n",
    "\n",
    "TODO:\n",
    "* Compute a confidence interval via the crowdMOS algorithm. The current confidence algorithm is more naive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This CSV can be downloaded from MTurk after recieving results from a batch.\n",
    "PATH = '/Users/michaelp/Code/Text-to-Speech/disk/other/Batch_3829940_batch_results.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "\n",
    "import pandas\n",
    "\n",
    "data_frame = pandas.read_csv(PATH)\n",
    "\n",
    "display(Markdown('### Number Of Workers'))\n",
    "display(Markdown(str(len(data_frame['WorkerId'].unique()))))\n",
    "\n",
    "display(Markdown('### Number Of Hits'))\n",
    "display(Markdown(str(len(data_frame))))\n",
    "\n",
    "display(Markdown('### Data Denominations'))\n",
    "data_frame.groupby(['Input.name']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy\n",
    "\n",
    "from IPython.display import Audio\n",
    "from IPython.display import Markdown\n",
    "\n",
    "from IPython.display import FileLink\n",
    "\n",
    "def random_sample(data_frame, *args, num_samples=100, sort_by='Input.name'):\n",
    "    \"\"\" Randomly sample audio clips from the data frame.\n",
    "    \"\"\"\n",
    "    if len(data_frame) == 0 or num_samples == 0:\n",
    "        return\n",
    "    \n",
    "    display(Markdown('### Random Sample'))\n",
    "    for i, row in data_frame.sample(n=min(num_samples, len(data_frame))).sort_values(by=[sort_by]).iterrows():\n",
    "        display(Markdown('**Index:** ' + str(i) + \n",
    "                         '  |  **Process Name:** ' + row['Input.name'] + \n",
    "                         '  |  **Speaker:** ' + row['Input.speaker'] ))\n",
    "        display(Markdown('**Text:** \"' + row['Input.text'] + '\"'))\n",
    "        for key in args:\n",
    "            display(Markdown('**%s:** ' % key + str(row[key])))\n",
    "        display(Audio(str(row['Input.audio_path'])))\n",
    "        display(Markdown('\\n\\n ___'))\n",
    "        display()\n",
    "        \n",
    "random_sample(data_frame, num_samples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add MOS Column\n",
    "\n",
    "We need to first convert the label to an MOS score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def label_to_score(label):\n",
    "    \"\"\" Get the MOS score from the MOS label.\n",
    "    \"\"\"\n",
    "    tokens = label.lower().split()\n",
    "    if 'excellent' in tokens:\n",
    "        return 5.0\n",
    "    elif 'good' in tokens:\n",
    "        return 4.0\n",
    "    elif 'fair' in tokens:\n",
    "        return 3.0\n",
    "    elif 'poor' in tokens:\n",
    "        return 2.0\n",
    "    elif 'bad' in tokens:\n",
    "        return 1.0\n",
    "    raise ValueError()\n",
    "\n",
    "data_frame['MOS'] = data_frame['Answer.audio-naturalness.label'].apply(label_to_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task Completion Time\n",
    "\n",
    "It's useful to guage the time it takes to compelete a task to determine fair worker pay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This may be useful for determining the time taken to complete a task.\n",
    "display(Markdown('### Density of time taken to complete a task:'))\n",
    "# NOTE: Remove outliers.\n",
    "# For example, it doesn't make sense to take 250 (4 minutes) seconds to review a 10 second clip. Workers that take\n",
    "# so long to complete a task, may be working on multiple tasks at a time.\n",
    "# NOTE (Michael 07-25-2019): From a previous analysis it looks like many workers are able to complete a task in\n",
    "# 3 - 4x the audio time.\n",
    "most_time = data_frame['WorkTimeInSecondsPerAudioSecond'][data_frame['WorkTimeInSecondsPerAudioSecond'] < 6]\n",
    "most_time.plot.kde(bw_method=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Bad Data\n",
    "\n",
    "Before we analyze our MOS scores, we are first going to filter out any bad data submitted to us.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter By Audio Length\n",
    "\n",
    "This removes any data submitted by workers that didn't listen to the entire audio clip based on Amazon's \n",
    "`WorkTimeInSeconds` metric. This filter was inspired by the \"crowdMOS\" paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "# Temporary addition to combine 'Input.audio_length' and 'Input.audio_length_in_seconds' together.\n",
    "def combine_columns(row):\n",
    "    return row[1] if numpy.isnan(row[0]) else row[0]\n",
    "\n",
    "data_frame['Input.audio_length_in_seconds'] = data_frame[\n",
    "    ['Input.audio_length', 'Input.audio_length_in_seconds']].apply(combine_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame['WorkTimeInSecondsPerAudioSecond'] = (\n",
    "    data_frame['WorkTimeInSeconds'] / data_frame['Input.audio_length_in_seconds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers_to_remove = list(set(data_frame[data_frame['WorkTimeInSecondsPerAudioSecond'] < 1.0]['WorkerId']))\n",
    "rows_to_remove = data_frame['WorkerId'].isin(workers_to_remove)\n",
    "display(Markdown('### Results'))\n",
    "display(Markdown('%d workers had a mininum `WorkTimeInSecondsPerAudioSecond` ' % len(workers_to_remove) +\n",
    "                 'less than the audio clip length and completed %d hits.' % rows_to_remove.sum()))\n",
    "random_sample(data_frame[data_frame['WorkTimeInSecondsPerAudioSecond'] < 1.0], 'WorkTimeInSecondsPerAudioSecond',\n",
    "              'Input.audio_length_in_seconds', num_samples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = data_frame[~rows_to_remove]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter By Ground Truth\n",
    "\n",
    "The ground truth audio should recieve a high score. A worker misunderstood the task if they graded our ground truth \n",
    "with a low score such as 'Poor - Mostly unnatural speech'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria = ((data_frame['Answer.audio-naturalness.label'] == 'Poor - Mostly unnatural speech') & \n",
    "            (data_frame['Input.type'] == 'gold'))\n",
    "workers_to_remove = list(set(data_frame[criteria]['WorkerId']))\n",
    "rows_to_remove = data_frame['WorkerId'].isin(workers_to_remove)\n",
    "display(Markdown('#### Results'))\n",
    "display(Markdown('%d workers rated a ground truth clip `Poor - Mostly unnatural speech` ' % len(workers_to_remove) +\n",
    "                 'and completed %d hits.' % rows_to_remove.sum()))\n",
    "random_sample(data_frame[criteria], 'WorkTimeInSecondsPerAudioSecond',\n",
    "              'Answer.audio-naturalness.label', num_samples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = data_frame[~rows_to_remove]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter By Past Experience\n",
    "\n",
    "Amazon provides us a \"Life Time Approval Rate\" for each worker. We can filter out any workers we have rejected in the past."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_past_approval = {}\n",
    "for key, value in data_frame.groupby('WorkerId')['LifetimeApprovalRate'].unique().iteritems():\n",
    "    assert len(value) == 1, 'There must be only one \"LifetimeApprovalRate\" per worker.'\n",
    "    value = value[0]\n",
    "    value = str(value).split('% (')  # Example Value: \"0% (0/0)\"\n",
    "    approval_rate = float(value[0])\n",
    "    num_approved, total_hits  = tuple(value[1][:-1].split('/'))\n",
    "    num_approved, total_hits = float(num_approved), float(total_hits)\n",
    "    assert (approval_rate == 0 and total_hits == 0) or (num_approved / total_hits) * 100 == approval_rate\n",
    "    worker_past_approval[key] = {\n",
    "        'num_approved': num_approved,\n",
    "        'total_hits': total_hits,\n",
    "        'approval_rate': approval_rate,\n",
    "    }\n",
    "\n",
    "workers_to_remove = [k for k, v in worker_past_approval.items() if v['total_hits'] != v['num_approved']]  \n",
    "rows_to_remove = data_frame['WorkerId'].isin(workers_to_remove)\n",
    "display(Markdown('#### Results'))\n",
    "display(Markdown('%d workers were rejected in the past ' % len(workers_to_remove) +\n",
    "                 'and completed %d hits.' % rows_to_remove.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = data_frame[~rows_to_remove]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter By Experience\n",
    "\n",
    "We know that this task may take sometime to get used to; therefore, it makes sense to filter our workers that have \n",
    "only graded a small number of clips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 5\n",
    "\n",
    "def combine_hits(row):\n",
    "    return row[0] + worker_past_approval[row.name]['total_hits']\n",
    "\n",
    "worker_total_hits = data_frame.groupby('WorkerId').size().to_frame(0)\n",
    "worker_total_hits[0] = worker_total_hits.apply(combine_hits, axis=1)\n",
    "workers_to_remove = list(worker_total_hits[worker_total_hits[0] < cutoff].index.unique())\n",
    "rows_to_remove = data_frame['WorkerId'].isin(workers_to_remove)\n",
    "display(Markdown('#### Results'))\n",
    "display(Markdown('%d workers completed less than %d hits ever ' % (len(workers_to_remove), cutoff) +\n",
    "                 'and completed %d hits in total' % rows_to_remove.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = data_frame[~rows_to_remove]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter by Speaker\n",
    "\n",
    "We only want to include the speakers that'll be used by our users on the website for analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers_to_remove = ['Judy Bieber', 'Mary Ann', 'Linda Johnson']\n",
    "filter_ = data_frame['Input.speaker'].isin(speakers_to_remove)\n",
    "data_frame = data_frame[~filter_]\n",
    "display(Markdown('#### Results'))\n",
    "display(Markdown('Removing %d hits.' % filter_.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter by Rejection\n",
    "\n",
    "Amazon provides a method for us to \"Reject\" a hit. We should filter out rejected hits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_ = data_frame['Reject'].notnull()\n",
    "data_frame = data_frame[~filter_]\n",
    "display(Markdown('#### Results'))\n",
    "display(Markdown('Removing %d hits.' % filter_.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Ground Truth vs Synthetic Filter\n",
    "\n",
    "We can filter our workers that score the ground truth samples lower than the synthetic samples, potentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown('#### Ground Truth vs Synthetic MOS'))\n",
    "merged = pandas.merge(\n",
    "    data_frame[data_frame['Input.type'] != 'gold'].groupby('WorkerId')['MOS'].describe()[['mean', 'count', 'std']],\n",
    "    data_frame[data_frame['Input.type'] == 'gold'].groupby('WorkerId')['MOS'].describe()[['mean', 'count', 'std']], \n",
    "    on='WorkerId',\n",
    "    suffixes=('_synthetic', '_ground_truth'))\n",
    "merged['gap'] = merged['mean_ground_truth'] - merged['mean_synthetic']\n",
    "merged.sort_values(by=['gap'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill this in manually for workers to remove based on the `gap` column with considersation for other statistics.\n",
    "workers_to_remove = []  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_ = data_frame['WorkerId'].isin(workers_to_remove)\n",
    "data_frame = data_frame[~filter_]\n",
    "display(Markdown('#### Results'))\n",
    "display(Markdown('Removing %d hits.' % filter_.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Filter\n",
    "\n",
    "From a random sample of the scores provided, are their any workers that are submitting poor results consistently?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 0 # Set this appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sample(data_frame, 'WorkerId', 'Answer.audio-naturalness.label', num_samples=num_samples, sort_by='WorkerId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers_to_remove = []\n",
    "filter_ = data_frame['WorkerId'].isin(workers_to_remove)\n",
    "data_frame = data_frame[~filter_]\n",
    "display(Markdown('#### Results'))\n",
    "display(Markdown('Removing %d hits.' % filter_.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Data Subset\n",
    "\n",
    "You'll want to select a subset of the data to analayze from here on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown('### Stats'))\n",
    "display(Markdown('#### Number Of Workers'))\n",
    "display(Markdown(str(len(data_frame['WorkerId'].unique()))))\n",
    "\n",
    "display(Markdown('#### Number Of Hits'))\n",
    "display(Markdown(str(len(data_frame))))\n",
    "\n",
    "display(Markdown('#### Data Denominations'))\n",
    "data_frame.groupby(['Input.name']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select your data subset here\n",
    "subset = data_frame[data_frame['Input.name'].isin(['ground-truth'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import scipy.stats\n",
    "\n",
    "def sample_mean_confidence_interval(data, confidence=0.95):\n",
    "    \"\"\"\n",
    "    NOTE: This is a similar approach to computing a confidence interval as the Tacotron 2 approach.\n",
    "    Inspired by: https://stackoverflow.com/questions/15033511/compute-a-confidence-interval-from-sample-data\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    a, b = scipy.stats.t.interval(confidence, len(data)-1, loc=np.mean(data), scale=scipy.stats.sem(data))\n",
    "    return np.mean(data), (b - a) / 2\n",
    "\n",
    "display(Markdown('### 95 Percent Confidence Interval \\n%f ± %f' % \n",
    "    sample_mean_confidence_interval(subset[\"MOS\"].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown('### MOS Distribution'))\n",
    "pandas.value_counts(data_frame['MOS']).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown('### Speaker MOS Score Distribution'))\n",
    "data_frame.groupby('Input.speaker')['MOS'].describe().sort_values(by=['mean'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
