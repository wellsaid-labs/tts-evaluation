{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze MTurk MOS Scores\n",
    "\n",
    "This repository analyzes MOS scores from MTurk.\n",
    "\n",
    "TODO:\n",
    "* Compute a confidence interval via the crowdMOS algorithm. The current confidence algorithm is more naive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This CSV can be downloaded from MTurk after receiving results from a batch.\n",
    "PATH = '~/Downloads/Batch_XXXXXXX_batch_results.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "\n",
    "import pandas\n",
    "\n",
    "data_frame = pandas.read_csv(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_data_summary(df):\n",
    "    display(Markdown('### Data Frame Stats'))\n",
    "    display(Markdown('**Number Of Workers**: ' + str(len(data_frame['WorkerId'].unique()))))\n",
    "    display(Markdown('**Number Of Hits**: ' + str(len(data_frame))))\n",
    "    display(Markdown('**Data Denominations**\\n'))\n",
    "    print(df.groupby(['Input.name']).size())\n",
    "\n",
    "display_data_summary(data_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy\n",
    "\n",
    "from IPython.display import Audio\n",
    "from IPython.display import Markdown\n",
    "\n",
    "from IPython.display import FileLink\n",
    "\n",
    "def random_sample(data_frame, *args, num_samples=100, sort_by='Input.name'):\n",
    "    \"\"\" Randomly sample audio clips from the data frame.\n",
    "    \"\"\"\n",
    "    if len(data_frame) == 0 or num_samples == 0:\n",
    "        return\n",
    "    \n",
    "    display(Markdown('### Random Sample'))\n",
    "    for i, row in data_frame.sample(n=min(num_samples, len(data_frame))).sort_values(by=[sort_by]).iterrows():\n",
    "        display(Markdown('**Index:** ' + str(i) + \n",
    "                         '  |  **Process Name:** ' + row['Input.name'] + \n",
    "                         '  |  **Speaker:** ' + row['Input.speaker'] ))\n",
    "        display(Markdown('**Text:** \"' + row['Input.text'] + '\"'))\n",
    "        for key in args:\n",
    "            display(Markdown('**%s:** ' % key + str(row[key])))\n",
    "        display(Audio(str(row['Input.audio_path'])))\n",
    "        display(Markdown('\\n\\n ___'))\n",
    "        display()\n",
    "        \n",
    "random_sample(data_frame, num_samples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task Completion Time\n",
    "\n",
    "It's useful to guage the time it takes to compelete a task to determine fair worker pay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "# Temporary addition to combine 'Input.audio_length' and 'Input.audio_length_in_seconds' together.\n",
    "def combine_columns(row):\n",
    "    return row[1] if numpy.isnan(row[0]) else row[0]\n",
    "\n",
    "data_frame['Input.audio_length_in_seconds'] = data_frame[\n",
    "    ['Input.audio_length', 'Input.audio_length_in_seconds']].apply(combine_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame['WorkTimeInSecondsPerAudioSecond'] = (\n",
    "    data_frame['WorkTimeInSeconds'] / data_frame['Input.audio_length_in_seconds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# NOTE: This may be useful for determining the time taken to complete a task.\n",
    "display(Markdown('### Density of time taken to complete a HIT:'))\n",
    "# NOTE: Remove outliers.\n",
    "# For example, it doesn't make sense to take 250 (4 minutes) seconds to review a 10 second clip. Workers that take\n",
    "# so long to complete a task, may be working on multiple tasks at a time.\n",
    "# NOTE (Michael 07-25-2019): From a previous analysis it looks like many workers are able to complete a task in\n",
    "# 3 - 4x the audio time.\n",
    "most_time = data_frame['WorkTimeInSecondsPerAudioSecond'][data_frame['WorkTimeInSecondsPerAudioSecond'] < 6]\n",
    "most_time.plot.kde(bw_method=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Score Column\n",
    "\n",
    "We need to first convert the label to numeric score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def label_to_score(label):\n",
    "    \"\"\" Get the numeric score from the qualitative label.\n",
    "    \"\"\"\n",
    "    tokens = label.lower().split()\n",
    "    if 'excellent' in tokens:\n",
    "        return 5.0\n",
    "    elif 'good' in tokens:\n",
    "        return 4.0\n",
    "    elif 'fair' in tokens:\n",
    "        return 3.0\n",
    "    elif 'poor' in tokens:\n",
    "        return 2.0\n",
    "    elif 'bad' in tokens:\n",
    "        return 1.0\n",
    "    raise ValueError()\n",
    "\n",
    "data_frame['Score'] = data_frame['Answer.audio-naturalness.label'].apply(label_to_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Bad Data\n",
    "\n",
    "Before we analyze our scores, we are first going to filter out any bad data submitted to us.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter By Past Experience\n",
    "\n",
    "Amazon provides us a \"Life Time Approval Rate\" for each worker. We can filter out any workers we have rejected in the past."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_past_approval = {}\n",
    "for key, value in data_frame.groupby('WorkerId')['LifetimeApprovalRate'].unique().iteritems():\n",
    "    assert len(value) == 1, 'There must be only one \"LifetimeApprovalRate\" per worker.'\n",
    "    value = value[0]\n",
    "    value = str(value).split('% (')  # Example Value: \"0% (0/0)\"\n",
    "    approval_rate = float(value[0])\n",
    "    num_approved, total_hits  = tuple(value[1][:-1].split('/'))\n",
    "    num_approved, total_hits = float(num_approved), float(total_hits)\n",
    "    assert (approval_rate == 0 and total_hits == 0) or (num_approved / total_hits) * 100 == approval_rate\n",
    "    worker_past_approval[key] = {\n",
    "        'num_approved': num_approved,\n",
    "        'total_hits': total_hits,\n",
    "        'approval_rate': approval_rate,\n",
    "    }\n",
    "\n",
    "workers_to_remove = [k for k, v in worker_past_approval.items() if v['total_hits'] != v['num_approved']]  \n",
    "rows_to_remove = data_frame['WorkerId'].isin(workers_to_remove)\n",
    "display(Markdown('#### Results'))\n",
    "display(Markdown('%d workers were rejected in the past ' % len(workers_to_remove) +\n",
    "                 'and completed %d hits.' % rows_to_remove.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = data_frame[~rows_to_remove]\n",
    "if len(workers_to_remove) > 0:\n",
    "    display_data_summary(data_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter By Experience\n",
    "\n",
    "We know that this task may take some time to get used to; therefore, it makes sense to filter out workers that have \n",
    "only graded a small number of clips. You can experiment using the [Analyze MOS v MTurk Worker Experience](https://github.com/wellsaid-labs/Text-to-Speech/blob/2f8d9492efee1458ceefdd3b9886564bc54bc743/notebooks/QA%20Datasets/Analyze%20MOS%20v%20MTurk%20Worker%20Experience.ipynb) notebook to find a good cutoff value. In this example, we chose 5 as the required minimum number of HITs to be completed by a worker in order to be considered in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 5\n",
    "\n",
    "def combine_hits(row):\n",
    "    return row[0] + worker_past_approval[row.name]['total_hits']\n",
    "\n",
    "worker_total_hits = data_frame.groupby('WorkerId').size().to_frame(0)\n",
    "worker_total_hits[0] = worker_total_hits.apply(combine_hits, axis=1)\n",
    "workers_to_remove = list(worker_total_hits[worker_total_hits[0] < cutoff].index.unique())\n",
    "rows_to_remove = data_frame['WorkerId'].isin(workers_to_remove)\n",
    "display(Markdown('#### Results'))\n",
    "display(Markdown('%d workers completed less than %d hits ever ' % (len(workers_to_remove), cutoff) +\n",
    "                 'and completed %d hits in total' % rows_to_remove.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = data_frame[~rows_to_remove]\n",
    "if len(workers_to_remove) > 0:\n",
    "    display_data_summary(data_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter by Rejection\n",
    "\n",
    "Amazon provides a method for us to \"Reject\" a hit. We should filter out rejected hits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_ = data_frame['Reject'].notnull()\n",
    "data_frame = data_frame[~filter_]\n",
    "display(Markdown('#### Results'))\n",
    "display(Markdown('Removing %d hits.' % filter_.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter by Speaker\n",
    "\n",
    "We only want to include for analysis the speakers that will be used by our users on the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "speakers_to_remove = ['Judy Bieber', 'Mary Ann', 'Linda Johnson']\n",
    "filter_ = data_frame['Input.speaker'].isin(speakers_to_remove)\n",
    "data_frame = data_frame[~filter_]\n",
    "display(Markdown('#### Results'))\n",
    "display(Markdown('Removing %d hits.' % filter_.sum()))\n",
    "display_data_summary(data_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter By Audio Length\n",
    "\n",
    "This removes any data submitted by workers that didn't listen to the entire audio clip based on Amazon's \n",
    "`WorkTimeInSeconds` metric. This filter was inspired by the \"crowdMOS\" paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers_to_remove = list(set(data_frame[data_frame['WorkTimeInSecondsPerAudioSecond'] < 1.0]['WorkerId']))\n",
    "rows_to_remove = data_frame['WorkerId'].isin(workers_to_remove)\n",
    "display(Markdown('### Results'))\n",
    "display(Markdown('%d workers had a mininum `WorkTimeInSecondsPerAudioSecond` ' % len(workers_to_remove) +\n",
    "                 'less than the audio clip length and completed %d hits.' % rows_to_remove.sum()))\n",
    "random_sample(data_frame[data_frame['WorkTimeInSecondsPerAudioSecond'] < 1.0], 'WorkTimeInSecondsPerAudioSecond',\n",
    "              'Input.audio_length_in_seconds', num_samples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = data_frame[~rows_to_remove]\n",
    "display_data_summary(data_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter By Ground Truth\n",
    "\n",
    "The ground truth audio should receive a high score. A worker misunderstood the task if they graded our ground truth \n",
    "with a low score such as 'Poor - Mostly unnatural speech'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria = ((data_frame['Answer.audio-naturalness.label'] == 'Poor - Mostly unnatural speech') & \n",
    "            (data_frame['Input.type'] == 'gold'))\n",
    "workers_to_remove = list(set(data_frame[criteria]['WorkerId']))\n",
    "rows_to_remove = data_frame['WorkerId'].isin(workers_to_remove)\n",
    "display(Markdown('#### Results'))\n",
    "display(Markdown('%d workers rated a ground truth clip `Poor - Mostly unnatural speech` ' % len(workers_to_remove) +\n",
    "                 'and completed %d hits.' % rows_to_remove.sum()))\n",
    "random_sample(data_frame[criteria], 'WorkTimeInSecondsPerAudioSecond',\n",
    "              'Answer.audio-naturalness.label', num_samples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = data_frame[~rows_to_remove]\n",
    "display_data_summary(data_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Ground Truth vs Synthetic Filter\n",
    "\n",
    "We can filter out workers that score the ground truth samples lower than the synthetic samples, potentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown('#### Ground Truth vs Synthetic MOS'))\n",
    "merged = pandas.merge(\n",
    "    data_frame[data_frame['Input.type'] != 'gold'].groupby('WorkerId')['Score'].describe()[['mean', 'count', 'std']],\n",
    "    data_frame[data_frame['Input.type'] == 'gold'].groupby('WorkerId')['Score'].describe()[['mean', 'count', 'std']], \n",
    "    on='WorkerId',\n",
    "    suffixes=('_synthetic', '_ground_truth'))\n",
    "merged['gap'] = merged['mean_ground_truth'] - merged['mean_synthetic']\n",
    "merged.sort_values(by=['gap'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill this in manually for workers to remove based on the `gap` column with considersation for other statistics.\n",
    "workers_to_remove = []  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_ = data_frame['WorkerId'].isin(workers_to_remove)\n",
    "data_frame = data_frame[~filter_]\n",
    "display(Markdown('#### Results'))\n",
    "display(Markdown('Removing %d hits.' % filter_.sum()))\n",
    "display_data_summary(data_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Filter\n",
    "\n",
    "From a random sample of the scores provided, are their any workers that are submitting poor results consistently?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10 # Set this appropriately.\n",
    "workers_to_sample = []\n",
    "filter_frame = data_frame.copy()[data_frame['WorkerId'].isin(workers_to_sample)]\n",
    "random_sample(filter_frame, 'WorkerId', 'Answer.audio-naturalness.label', num_samples=num_samples, sort_by='WorkerId')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the findings from above samples, remove select workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers_to_remove = [] # Set this based on investigation\n",
    "filter_ = data_frame['WorkerId'].isin(workers_to_remove)\n",
    "data_frame = data_frame[~filter_]\n",
    "display(Markdown('#### Results'))\n",
    "display(Markdown('Removing %d hits.' % filter_.sum()))\n",
    "display_data_summary(data_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Data Subsets\n",
    "\n",
    "You'll want to select various subsets of the data to analayze from here on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown('### Stats'))\n",
    "display(Markdown('#### Number Of Workers'))\n",
    "display(Markdown(str(len(data_frame['WorkerId'].unique()))))\n",
    "\n",
    "display(Markdown('#### Number Of Hits'))\n",
    "display(Markdown(str(len(data_frame))))\n",
    "\n",
    "display(Markdown('#### Data Denominations'))\n",
    "data_frame.groupby(['Input.name']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select your data subsets here\n",
    "truth = data_frame[data_frame['Input.name'].isin(['ground-truth'])]\n",
    "trained =  data_frame[data_frame['Input.name'].isin(['YOUR-EXPERIMENT-NAME'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import scipy.stats\n",
    "\n",
    "def sample_mean_confidence_interval(data, confidence=0.95):\n",
    "    \"\"\"\n",
    "    NOTE: This is a similar approach to computing a confidence interval as the Tacotron 2 approach.\n",
    "    Inspired by: https://stackoverflow.com/questions/15033511/compute-a-confidence-interval-from-sample-data\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    a, b = scipy.stats.t.interval(confidence, len(data)-1, loc=np.mean(data), scale=scipy.stats.sem(data))\n",
    "    return np.mean(data), (b - a) / 2\n",
    "\n",
    "x_order = [5.0, 4.0, 3.0, 2.0, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown('### GROUND-TRUTH: 95% Confidence Interval + Opinion Score Distribution \\n%f ± %f' % \n",
    "    sample_mean_confidence_interval(truth['Score'].tolist())))\n",
    "pandas.value_counts(truth['Score']).reindex(x_order).plot(kind='bar', color='yellowgreen').set(xlabel=\"Score\", ylabel=\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown('### TRAINED: 95% Confidence Interval + Opinion Score Distribution \\n%f ± %f' % \n",
    "    sample_mean_confidence_interval(trained['Score'].tolist())))\n",
    "pandas.value_counts(trained['Score']).reindex(x_order).plot(kind='bar', color='orange').set(xlabel=\"Score\", ylabel=\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_compare(df, colors, legend):\n",
    "    i = 0\n",
    "    for frame in df:\n",
    "        frame.set_index('Score')\n",
    "        print(len(df))\n",
    "        pandas.value_counts(frame['Score']).reindex(x_order).plot(kind=\"bar\",\n",
    "                                                            color=colors[i], width=.75/len(df), \n",
    "                                                            position=i+1, stacked=False, \n",
    "                                                            label=legend[i], legend=True, \n",
    "                                                            figsize=(8, 6)).set(xlabel=\"Score\", ylabel=\"Count\")\n",
    "        i+=1\n",
    "\n",
    "# Create a list of dataframes to compare; assign colors and legend titles to each (maintain the ordering)\n",
    "# You can see a list of python colors here: https://python-graph-gallery.com/python-colors/\n",
    "compare_dfs = [trained1, truth]\n",
    "colors = ['orange', 'yellowgreen']\n",
    "legend = ['TRAINED', 'GROUND-TRUTH']\n",
    "\n",
    "display(Markdown('### COMPARE: Ground Truth v. Trained Opinion Score Distribution \\n'))\n",
    "bar_compare(compare_dfs, colors, legend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown('### GROUND-TRUTH: Speaker MOS Distribution'))\n",
    "truth.groupby('Input.speaker')['Score'].describe().sort_values(by=['mean'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown('### TRAINED: Speaker MOS Distribution'))\n",
    "trained.groupby('Input.speaker')['Score'].describe().sort_values(by=['mean'], ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
