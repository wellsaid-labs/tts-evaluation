from torch import nn

import torch
import librosa
import IPython

from src.utils.configurable import add_config
from src.utils.configurable import configurable
from src.utils import AnomalyDetector


def set_hparams():
    """ Using the ``configurable`` module set the hyperparameters for the source code.
    """
    torch.optim.Adam.__init__ = configurable(torch.optim.Adam.__init__)
    nn.modules.batchnorm._BatchNorm.__init__ = configurable(
        nn.modules.batchnorm._BatchNorm.__init__)
    add_config({
        # NOTE: `momentum=0.01` to match Tensorflow defaults
        'torch.nn.modules.batchnorm._BatchNorm.__init__': {
            'momentum': 0.01,
        },
        # SOURCE (Tacotron 2):
        # We use the Adam optimizer [29] with β1 = 0.9, β2 = 0.999
        'torch.optim.adam.Adam.__init__': {
            'betas': (0.9, 0.999),
            'amsgrad': True,
            'lr': 10**-3
        }
    })

    # SOURCE (Tacotron 2):
    # The convolutional layers in the network are regularized using dropout [25] with probability
    # 0.5, and LSTM layers are regularized using zoneout [26] with probability 0.1
    convolution_dropout = 0.5
    lstm_dropout = 0.1

    # Hidden size of the feature representation generated by encoder.
    encoder_hidden_size = 512

    # SOURCE (Tacotron 2):
    # The prediction from the previous time step is first passed through a small
    # pre-net containing 2 fully connected layers of 256 hidden ReLU units.
    pre_net_hidden_size = 256

    # SOURCE (Tacotron 2):
    # Attention probabilities are computed after projecting inputs and location
    # features to 128-dimensional hidden representations.
    attention_hidden_size = 128

    # SOURCE (Tacotron 1):
    # We use 24 kHz sampling rate for all experiments.
    sample_rate = 24000

    get_log_mel_spectrogram = {
        'sample_rate': sample_rate,
        # SOURCE (Tacotron 2):
        # mel spectrograms are computed through a shorttime Fourier transform (STFT)
        # using a 50 ms frame size, 12.5 ms frame hop, and a Hann window function.
        'frame_size': 1200,  # 50ms * 24,000 / 1000 == 1200
        'frame_hop': 300,  # 12.5ms * 24,000 / 1000 == 300
        'window': 'hann',

        # SOURCE (Tacotron 1):
        # 2048-point Fourier transform
        'fft_length': 2048,

        # SOURCE (Tacotron 2):
        # Prior to log compression, the filterbank output magnitudes are clipped to a
        # minimum value of 0.01 in order to limit dynamic range in the logarithmic
        # domain.
        'min_magnitude': 0.01,
    }

    # SOURCE (Tacotron 2):
    # We transform the STFT magnitude to the mel scale using an 80 channel mel
    # filterbank spanning 125 Hz to 7.6 kHz, followed by log dynamic range
    # compression.
    # SOURCE (Tacotron 2 Author):
    # Google mentioned they settled on [20, 12000] with 128 filters in Google Chat.
    lower_hertz = 20
    upper_hertz = sample_rate / 2
    frame_channels = 128

    # SOURCE: Efficient Neural Audio Synthesis
    # The WaveRNN model is a single-layer RNN with a dual softmax layer that is
    # designed to efficiently predict 16-bit raw audio samples.
    bits = 16

    librosa.effects.trim = configurable(librosa.effects.trim)
    IPython.display.Audio.__init__ = configurable(IPython.display.Audio.__init__)

    # TODO: Add option to instead of strings to use direct references.
    add_config({
        'librosa.effects.trim': {
            'frame_length': get_log_mel_spectrogram['frame_size'],
            'hop_length': get_log_mel_spectrogram['frame_hop'],
            # NOTE: Manually determined to be a adequate cutoff for Linda Johnson via:
            # ``notebooks/Stripping Silence.ipynb``
            'top_db': 50
        },
        'IPython.lib.display.Audio.__init__.rate': sample_rate,
        'src': {
            'datasets.lj_speech.lj_speech_dataset': {
                'resample': sample_rate,
                # NOTE: ``Signal Loudness Distribution`` notebook shows that LJ Speech is biased
                # concerning the loudness and ``norm=True`` unbiases this. In addition, norm
                # also helps smooth out the distribution in notebook ``Signal Energy Distribution``.
                # While, ``loudness=True`` does not help.
                'norm': True,
                'loudness': False,
                # NOTE: Guard to reduce clipping during resampling
                'guard': True,
                # NOTE: Highpass and lowpass filter to ensure Wav is consistent with Spectrogram.
                'lower_hertz': None,
                'upper_hertz': None,
            },
            'audio': {
                # SOURCE (Wavenet):
                # To make this more tractable, we first apply a µ-law companding transformation
                # (ITU-T, 1988) to the data, and then quantize it to 256 possible values
                'read_audio.sample_rate': sample_rate,
                'get_log_mel_spectrogram': get_log_mel_spectrogram,
                'griffin_lim': {
                    'frame_size': get_log_mel_spectrogram['frame_size'],
                    'frame_hop': get_log_mel_spectrogram['frame_hop'],
                    'fft_length': get_log_mel_spectrogram['fft_length'],
                    'window': get_log_mel_spectrogram['window'],
                    'sample_rate': sample_rate,
                    # SOURCE (Tacotron 1):
                    # We found that raising the predicted magnitudes by a power of 1.2 before
                    # feeding to Griffin-Lim reduces artifacts
                    'power': 1.20,
                    # SOURCE (Tacotron 1):
                    # We observed that Griffin-Lim converges after 50 iterations (in fact, about 30
                    # iterations seems to be enough), which is reasonably fast.
                    'iterations': 30,
                },
                'mel_filters': {
                    'fft_length': get_log_mel_spectrogram['fft_length'],
                    'sample_rate': sample_rate,
                    # SOURCE (Tacotron 2):
                    # We transform the STFT magnitude to the mel scale using an 80 channel mel
                    # filterbank spanning 125 Hz to 7.6 kHz, followed by log dynamic range
                    # compression.
                    'num_mel_bins': frame_channels,
                    'lower_hertz': lower_hertz,
                    'upper_hertz': upper_hertz,
                }
            },
            'feature_model': {
                'encoder.Encoder.__init__': {
                    'lstm_dropout': lstm_dropout,
                    'convolution_dropout': convolution_dropout,

                    # SOURCE (Tacotron 2):
                    # Input characters are represented using a learned 512-dimensional character
                    # embedding
                    'embedding_dim': 512,

                    # SOURCE (Tacotron 2):
                    # which are passed through a stack of 3 convolutional layers each containing
                    # 512 filters with shape 5 × 1, i.e., where each filter spans 5 characters
                    'num_convolution_layers': 3,
                    'num_convolution_filters': 512,
                    'convolution_filter_size': 5,

                    # SOURCE (Tacotron 2)
                    # The output of the final convolutional layer is passed into a single
                    # bi-directional [19] LSTM [20] layer containing 512 units (256) in each
                    # direction) to generate the encoded features.
                    'lstm_hidden_size': encoder_hidden_size,  # 512
                    'lstm_layers': 1,
                    'lstm_bidirectional': True,
                },
                'attention.LocationSensitiveAttention.__init__': {
                    'encoder_hidden_size': encoder_hidden_size,

                    # SOURCE (Tacotron 2):
                    # Attention probabilities are computed after projecting inputs and location
                    # features to 128-dimensional hidden representations.
                    'hidden_size': attention_hidden_size,

                    # SOURCE (Tacotron 2):
                    # Location features are computed using 32 1-D convolution filters of length
                    # 31.
                    'num_convolution_filters': 32,
                    'convolution_filter_size': 31,
                },
                'decoder.AutoregressiveDecoder.__init__': {
                    'frame_channels': frame_channels,
                    'pre_net_hidden_size': pre_net_hidden_size,
                    'encoder_hidden_size': encoder_hidden_size,
                    'lstm_dropout': lstm_dropout,
                    'attention_context_size': attention_hidden_size,

                    # SOURCE (Tacotron 2):
                    # The prenet output and attention context vector are concatenated and
                    # passed through a stack of 2 uni-directional LSTM layers with 1024 units.
                    'lstm_hidden_size': 1024,
                },
                'pre_net.PreNet.__init__': {
                    'frame_channels': frame_channels,

                    # SOURCE (Tacotron 2):
                    # The prediction from the previous time step is first passed through a small
                    # pre-net containing 2 fully connected layers of 256 hidden ReLU units.
                    'num_layers': 2,
                    'hidden_size': pre_net_hidden_size,

                    # SOURCE (Tacotron 2):
                    # In order to introduce output variation at inference time, dropout with
                    # probability 0.5 is applied only to layers in the pre-net of the
                    # autoregressive decoder.
                    'dropout': 0.5,
                },
                'post_net.PostNet.__init__': {
                    'frame_channels': frame_channels,
                    'convolution_dropout': convolution_dropout,

                    # SOURCE (Tacotron 2):
                    # Finally, the predicted mel spectrogram is passed
                    # through a 5-layer convolutional post-net which predicts a residual
                    # to add to the prediction to improve the overall reconstruction
                    'num_convolution_layers': 5,

                    # SOURCE (Tacotron 2):
                    # Each post-net layer is comprised of 512 filters with shape 5 × 1 with
                    # batch normalization, followed by tanh activations on all but the final
                    # layer
                    'num_convolution_filters': 512,
                    'convolution_filter_size': 5,
                },
                'model.SpectrogramModel.__init__': {
                    'encoder_hidden_size': encoder_hidden_size,
                    'frame_channels': frame_channels,
                }
            },
            'signal_model': {
                'upsample.ConditionalFeaturesUpsample.__init__': {
                    # SOURCE: Efficient Neural Audio Synthesis Author
                    # The author suggested adding 3 - 5 convolutions on top of WaveRNN.
                    # SOURCE:
                    # https://github.com/pytorch/examples/blob/master/super_resolution/model.py
                    # Upsampling layer is inspired by super resolution
                    'kernels': [(5, 5), (3, 3), (3, 3), (3, 3)],
                },
                'wave_rnn.WaveRNN': {
                    'infer': {
                        # SOURCE: Generating Sequences With Recurrent Neural Networks
                        # One problem with unbiased samples is that they tend to be difficult to
                        # read (partly because real handwriting is difficult to read, and partly
                        # because the network is an imperfect model). Intuitively, we would expect
                        # the network to give higher probability to good handwriting because it
                        # tends to be smoother and more predictable than bad handwriting. If this is
                        # true, we should aim to output more probable elements of Pr(x|c) if we want
                        # the samples to be easier to read.
                        # NOTE: Temperature is a concept from reinforcement learning to bias the
                        # softmax similar to the above idea.
                        'temperature': 1.0,
                        'argmax': False,
                    },
                    '__init__': {
                        'local_features_size': frame_channels,

                        # SOURCE: Efficient Neural Audio Synthesis
                        # The WaveRNN model is a single-layer RNN with a dual softmax layer that is
                        # designed to efficiently predict 16-bit raw audio samples.
                        'bits': bits,

                        # SOURCE: Efficient Neural Audio Synthesis
                        # We see that the WaveRNN with 896 units achieves NLL scores comparable to
                        # those of the largest WaveNet model
                        'hidden_size': 896,

                        # SOURCE: Tacotron 2
                        # only 2 upsampling layers are used in the conditioning stack instead of 3
                        # layers.
                        # SOURCE: Tacotron 2 Author Google Chat
                        # We upsample 4x with the layers and then repeat each value 75x
                        'upsample_num_filters': [64, 64, 32, 10],
                        'upsample_repeat': 30
                    }
                },
            },
            'bin.evaluate_signal_model.main.sample_rate': sample_rate,
            'bin.signal_model': {
                'train.Trainer.__init__': {
                    'sample_rate': sample_rate,
                    'min_rollback': 1,
                },
                '_utils.load_data.predicted': False,
                '_dataset.SignalDataset.__init__': {
                    # SOURCE: Efficient Neural Audio Synthesis
                    # The WaveRNN models are trained on sequences of 960 audio samples
                    'frame_size': int(900 / get_log_mel_spectrogram['frame_hop']),
                    'frame_pad': 5,
                }
            },
            'utils': {
                'visualize': {
                    'Tensorboard.add_audio.sample_rate': sample_rate,
                    'plot_waveform.sample_rate': sample_rate,
                    'plot_log_mel_spectrogram': {
                        'sample_rate': sample_rate,
                        'frame_hop': get_log_mel_spectrogram['frame_hop'],
                        'lower_hertz': lower_hertz,
                        'upper_hertz': upper_hertz,
                    },
                },
                'utils': {
                    'split_signal.bits': bits,
                    'combine_signal.bits': bits,
                    'AnomalyDetector.__init__': {
                        # NOTE: Determined empirically with the notebook:
                        # ``notebooks/Detecting Anomalies.ipynb``
                        'sigma': 6,
                        'beta': 0.98,
                        'type_': AnomalyDetector.TYPE_HIGH,
                    }
                }
            },
            'optimizer.AutoOptimizer.__init__': {
                # NOTE: Window size smoothing parameter is not super sensative.
                'window_size': 128
            }
        }
    })
