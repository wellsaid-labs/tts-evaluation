"""
The other idea is... pregenerate a million rows. This might be the easiest approach to deal with
all of this... It then depends on how much you think annotations matter. For context, the model
will go through the data 44 million times or more... So it'll repeat the data 44x.

Let's specify this a bit more...

_gcs_alignment_dataset_loader:
    - Download and load the dataset
    - Sample from the dataset (including splitting and shuffling)
    - TODO: Print the size of each dataset loaded.
    - TODO: Return the size of the dataset alongside the generator.
    - TODO: Handle distributed: Each process does not need download the dataset; however, it's fine
      if they all have their own copy of the data.
      The generator will need to be replicated accross workers in the `DataLoader`.

get_dataset:
    - TODO: Get each of the datasets with dataset loader.
    - TODO: Return a generator that runs non-generalizable (related to hyperparameters and unrelated to input_encoder)
      preprocessing on our datasets.
        - Sampling: The datasets should be sampled equally per speaker; however, we'll eventually
            want to transition away from that model so that we can take advantage of more data.
            We'll need to ensure the number of hours is balanced. We can just sample from
            the dataset with the least number of hours sampled so far...
        - Filtering: (Brainstorm)
          - If `audio_path` is not found
          - If there is no text
          - If there is no audio
          - Remove particular books or speakers
          - Too much audio or too little audio per alignment and it's phonetics.
          - Too many characters or too little characters per alignment.
          - Too quiet?
          - There are numbers
          - Bad alignments
        - Normalize audio w/ ffmpeg and the settings Rhyan sent me.
    - TODO: Using `HParams` we can parameterize the dataset generator function.

- The idea is... we need a function that'll preprocess single rows. It'll also use multiple
  workers to preprocess more rows faster. It'll cache the first 100,000 rows preprocessed.
  It won't start training until the first 100,000 rows are preprocessed. It'll fetch
  older rows of data, if there are no new rows of data to use.
- The concern is that... we cannot effectively cache the `_load_fn` because the `DataLoader`
  has multiple workers running `_load_fn`.
- The solution to that concern is... `_load_fn` still loads a single row. We have a sampler
  that requests 100,000 rows before it produces the first sample. Unfortunately, the samplers
  don't work that way... The samplers largely work with indicies rather than the actual data.
- An additional concern is that... the data loader will also need to replicate the generator,
  which is not feasible. We can't create the generator... at least yet.
- The solution is... `_load_fn` saves examples as they are tabulated under some unique key
  until the filesystem reaches 100,000 examples saved. Since the generator is random, the
  processes can run it indepedently and load data independently. The cache is loaded
  on startup, and each process grabs data based on the worker key.
  Instead of using `_load_fn` which is a "map-style dataset" we can use an "iterable style
  dataset". It better fits the situation.
- Question: How do we handle data transfer?
    - Should each function have it's own copy of the dataset? Likely not.
- Question: How do we handle vocabulary? Since the data is autogenerated... We can't go through
  the entire dataset. We could automatically add additional embeddings (depending on the input).
  We could also seperate a concept of the "data_generator" and the "data". There are similar paterns
  to this with IO. The generator is simply used for reading the data.
- Question: What preprocessing does HParams handle and what preprocessing does `_load_fn` handle?
  I think it depends on the principles of modularity. Which preprocessing is specific to the
  training process? Which is general to the dataset?
- Question: The sampler requires us to be able to grab a specific index of a dataset. Should we
  be using the sampler at all? We can integrate with the samplers a little bit... We can wrap
  an iterable with a mapping interface. Basically, if you request a particular index, then
  the generator will generate up to that index. Also, we'd want to remove that index afterwards
  so that we don't have a memory failure.
- IDEA: Instead of an `IterableDataset` we use a regular dataset. Given a particular index,
  it'll generate up to that index and return it. Any unused data will be stored in a dictionary.
  Everytime we lookup some data, it's deleted afterwards. Or there is a maximum number of indicies
  that can be stored, and it's implemented as a FIFO queue.
- QUESTION: Does the model have to take in tensors only? Can it take in a regular list? Can
  we batch a regular list of tensors? The tensors will then be expanded based on a word mapping
  afterwards...?
    - collate_fn: We can write a custom `collate_fn` in order to handle the batching.
    - `DistributedDataParallel`: Doesn't have any custom logic during the forward pass that'd
      prevent me from passing a list in.
- IDEA: The attention module is already able to operate efficiently with a batch of sequences.
  We could "batch" the word-level, character-level, and phone-level sequences. We could even
  reuse the same encoder between the character-level and phone-level sequences. I'm a bit more
  skeptical about using the word-level sequences with the same encoder since they are so different.
- IDEA: Instead of implementing a caching method for predicted spectrogram output, we could
  add a GAN to the spectrogram. I'd prefer that...

Trainer.__init__ or Model.__init__:
  - Iterate through the text, and create a vocab.
  - Cache the graphemes and spaCy, iterate, and create a vocab.
  - Iterate through the speakers, and create a vocab.
  - Create an embedding table for phonemes.
  - Create an embedding table for capitalization

data_loader#_load_fn:
  - TODO: The `DataLoader` instead of taking a `Dataset` and a `Sampler`. It'll take on
    an `IterableDataset`:
        - 1. It's given data (alignments / scripts / audio_path) and a generator function.
        (NOTE: We don't need this... except maybe for signal model training...)
        - 2. It preloads any existing data. (With respect to it's worker id?)
        - 3. Similar to worker, it starts a thread that runs the generator with a queue.
        - 4. Every iteration, if there are items in the queue it yeilds them; otherwise, it'll
             yeild a pre-existing item.
  - TODO: Implement a function that turns turns a sampler into an iterator. The sampler is able
    accept an iterator, and it returns an iterator. The function has a "sample with replacement"
    and "sample without replacement" mode. Use this iterator with the remaining samplers.
  - TODO: Get rid of the Oom sampler because it requires us to iterate through the entire dataset.
    We could try to iterate through a subset of the dataset, in order to prevent an OOM. Let's
    not worry about it for right now.
  - TODO: Get rid of DistributedBatchSampler because the data is randomly generated on each
    process; therefore, we don't need to segment it.
  - TODO: We can get rid of DeterministicSampler because the process will already be
    non-determinsitic.
  - TODO: Get rid of the balanced sampler, since the generator is already balanced by definition.
    Double check this...
  - TODO: Implement additional preprocessing:
    (For spectrogram performance, we should be okay. For signal model, we'll need a solution for
    predicted spectrogram. Ideally, we can just get rid of the need for a predicted spectrogram...
    by adding a GAN module to the spectrogram model.)
    - Preprocess with spaCy (and cache) (36ms) (Cache)
    - Average and rounded (to prevent overfitting) loudness (with ITU-R BS.1770-4) (17ms)
    - Average and rounded (to prevent overfitting) speed (seconds per phoneme).
    - Average and rounded (to prevent overfitting) pitch (with CREPE or SPICE or torchaudio) (671ms) (Don't Implement)
    - For a random number of transitions, compute the (rounded to prevent overfitting) pause
      time in seconds.
    - Lower case the text, and provide an extra feature with regard to capitalization. (in the input encoder?)
    - For a random number of words, provide the phonetic spelling. (522ms) (Cache)
    - Extract and provide any related metadata with regards to book or article.
    - Get spectrogram (38ms)
  - TODO: Following preprocessing, the process should return `SpectrogramModelTrainingRow` with:
      - loudness: torch.FloatTensor [num_tokens] (0 is maximum, -100 is minimum)
      - loudness_mask
      - speed: torch.FloatTensor [num_tokens] (0 is minmimum, 16 characters on average)
      - speed_mask
      - pausing: torch.FloatTensor [num_tokens] (0 is minimum, 1 or 2 seconds)
      - pausing_mask
      - capitalization: torch.LongTensor [num_tokens] (capitalized, not capitalized, not letter)
      - text: torch.LongTensor [num_tokens]
      - word_to_text: list of text spans
      - phoneme: list of phonetic spellings
      - word_vectors: torch.FloatTensor [num_words, word_vector_size]
      - contextual_word_vectors: torch.FloatTensor [num_words, word_vector_size]
      - speaker: torch.LongTensor [1]
      - spectrogram: torch.FloatTensor [num_frames, frame_channels]
      - spectrogram_mask: torch.FloatTensor [num_frames]
      - spectrogram_extended_mask: torch.FloatTensor [num_frames, frame_channels]
      - stop_token: torch.FloatTensor [num_frames]

Trainer#_do_loss_and_maybe_backwards:
  - TODO: Remove `expected_average_spectrogram_length` and replace it with a constant or calculate
    it from the first 100,000k or so examples.

Model#forward:
  - Accept loudness, speed, pausing, capitalization, text, word_to_text, phoneme, word_vectors,
    contextual_word_vectors, speaker
  - Linear layer for loudness, speed, pausing that are applied, if a value is present.
  - Capitalization embedding
  - Embedding for text
  - Embedding for phoneme
  - Learned padding vector between words. It's distinct from the other types of padding that
    we ignore.
  - Map the words vectors and phonemes to the text via word_to_text. Use PyTorch to interpolate,
    so they are the same size. Potentially... before interpolation, we could run the convolutions.
  - Loudness / speed / pausing should default to 0 with an appropriate loudness mask.

Worker:
  - Run spaCy on input for tokenization and for word_to_text vector.
  - Input encoder:
      - Grapheme to phoneme
      - Capitalization
      - Phonemes
  - Accept XML to create loudness, speed, pausing and phoneme.

TODO:
1. Print the size of each dataset loaded.
2. Ensure old datasets like LJ and M-AILABS are converted to the new format.
3. Weigh the datasets based
4. Preprocess Examples:
    1. Add

datasets/
    Within this module
"""
from collections import namedtuple
from enum import Enum
from math import ceil
from math import floor
from pathlib import Path

import json
import logging
import pprint
import random
import subprocess

from third_party import LazyLoader

librosa = LazyLoader('librosa', globals(), 'librosa')
pandas = LazyLoader('pandas', globals(), 'pandas')

from src.environment import DATA_PATH
from src.utils import cumulative_split
from src.utils import flatten
from src.utils import natural_keys

logger = logging.getLogger(__name__)
pprint = pprint.PrettyPrinter(indent=4)

# Args:
#     alignments (list of tuple(tuple(int, int), tuple(int, int))): List of alignments from
#         `script` to `audio_path`.
#     script (str): The script read in the voice over.
#     audio_path (pathlib.Path): The voice over.
#     speaker (Speaker): Speaker represented in voice over.
#     metadata (dict): Additional metadata associated with this example.
Example = namedtuple(
    'Example', ['alignments', 'script', 'audio_path', 'speaker', 'metadata'],
    defaults=(None, None, None, None, {}))


class Gender(Enum):
    FEMALE = 0
    MALE = 1


class Speaker(object):

    def __init__(self, name, gender):
        self.name = name
        self.gender = gender

    def __eq__(self, other):
        if isinstance(other, Speaker):
            return self.name == other.name and self.gender == other.gender

        # Learn more:
        # https://stackoverflow.com/questions/878943/why-return-notimplemented-instead-of-raising-notimplementederror
        return NotImplemented

    def __hash__(self):
        # Learn more:
        # https://computinglife.wordpress.com/2008/11/20/why-do-hash-functions-use-prime-numbers/
        return 32 * hash(self.name) + 97 * hash(self.gender)

    def __repr__(self):
        return '%s(name=\'%s\', gender=%s)' % (self.__class__.__name__, self.name, self.gender.name)


def _alignment_generator(data, max_seconds):
    """ Generate a `Example`s that are at most `max_seconds` long.

    Examples, representing a interval of time, are sampled uniformly over the audio file.
    Afterwards unaligned or aligned intervals that overlap with the boundaries are removed.

    NOTE:
    - Longer alignments or unalignments are less likely to be sampled because they are more
      likely to overlap with the boundary.
    - The length of the sampled interval is less likely to be the full `max_seconds` afterward
      any boundary segments are removed.

    TODO:
    - Look into removing these biases by including boundary intervals based on some statistical
    correction.
    - Log the alignment distribution in order to ensure it's relatively uniform.
    - Look into extending the maximum seconds by the size of the largest alignment, and then
      filtering out alignments that are longer than the max alignment.
    - Does this need extra tests? Yes. We should test small max legnth and alignments

    Args:
        data (list of Example): List of examples to sample from.
        max_seconds (float): The maximum interval length.

    Returns:
        (iterator of Example)
    """
    assert max_seconds > 0, 'The maximum interval length must be a positive number.'
    data = sorted(data, key=lambda e: e.alignments[0][1][0])
    if len(data) == 0:
        return iter([])
    min_ = data[0].alignments[0][1][0]
    max_ = data[-1].alignments[-1][1][1]
    lookup = [[] for _ in range(ceil(max_) + 1)]
    for i, example in enumerate(data):
        for j, alignment in enumerate(example.alignments):
            for k in range(int(floor(alignment[1][0])), int(ceil(alignment[1][0])) + 1):
                lookup[k].append((i, j))
    find = lambda i: data[i[0]].alignments[i[1]][1]  # Given the indicies return the audio span.
    while True:
        length = random.uniform(0.0, max_seconds)
        start = random.uniform(min_ - length, max_)
        end = min(max_, start + length)
        start = max(min_, start)
        slice_ = flatten(lookup[int(start):int(end) + 1])
        start = next((i for i in slice_ if find(i)[0] >= start and find(i)[1] >= start), None)
        end = next((i for i in reversed(slice_) if find(i)[0] <= end and find(i)[1] <= end), None)
        if start and end and start[0] == end[0] and find(end)[1] - find(start)[0] > 0:
            example = data[start[0]]
            yield example._replace(alignments=example.alignments[start[1]:end[1] + 1])


def _gcs_alignment_dataset_loader(root_directory_name,
                                  speaker,
                                  splits,
                                  directory=DATA_PATH,
                                  gcs_path='gs://wellsaid_labs_datasets/hilary_noriega',
                                  alignments_directory_name='alignments',
                                  recordings_directory_name='recordings',
                                  scripts_directory_name='scripts',
                                  text_column='Content',
                                  max_seconds=15):
    """ Load an alignment text-to-speech (TTS) dataset from GCS.

    TODO: Print dataset size with `seconds_to_string`.

    The structure of the dataset should be:
        - The file structure is similar to:
            {gcs_path}/
            ├── {alignments_directory_name}/  # Alignments between recordings and scripts
            │   ├── audio1.json
            │   └── ...
            ├── {recordings_directory_name}/  # Voice overs
            │   ├── audio1.wav                # NOTE: Most audio file formats are accepted.
            │   └── ...
            └── {scripts_directory_name}/     # Voice over scripts with related metadata
                ├── audio1-script.csv
                └── ...
        - The alignments, recordings, and scripts directory should contain the same number of
          similarly named files.
        - The dataset contain data representing only one speaker.

    Args:
        root_directory_name (str): Name of the directory inside `directory` to store data.
        speaker (src.datasets.Speaker): The speaker represented by this dataset.
        splits (list of int): The size of each dataset split in seconds. Iff the the total size
            is smaller than the dataset size, then an extra dataset split will be returned with
            the remaining data.
        directory (str or Path, optional): Directory to cache the dataset.
        gcs_path (str, optional): The base GCS path storing the data.
        alignments_gcs_path (str, optional): The name of the alignments directory on GCS.
        recordings_gcs_path (str, optional): The name of the voice over directory on GCS.
        scripts_gcs_path (str, optional): The name of the voice over script directory on GCS.
        text_column (str, optional): The voice over script column in the CSV script files.
        max_seconds (int, optional): The length of an example.

    Returns:
        (tuple of generator of Example): A generator of `Example`s is returned for each split.
    """
    logger.info('Loading `%s` speech dataset', root_directory_name)

    root = (Path(directory) / root_directory_name).absolute()
    root.mkdir(exist_ok=True)
    directories = [alignments_directory_name, recordings_directory_name, scripts_directory_name]
    directories = [root / d for d in directories]
    for directory, suffix in zip(directories, ('.json', '', '.csv')):
        directory.mkdir(exist_ok=True)
        command = 'gsutil -m cp -n %s/%s/*%s %s/' % (gcs_path, directory.name, suffix, directory)
        subprocess.run(command.split(), check=True)

    files = (sorted(d.iterdir(), key=lambda p: natural_keys(p.name)) for d in directories)
    examples = []
    for alignment_file_path, recording_file_path, script_file_path in zip(*tuple(files)):
        alignments = json.loads(alignment_file_path.read_text())
        scripts = pandas.read_csv(str(script_file_path.absolute()))
        assert len(scripts) == len(alignments), 'Expected equal number of scripts and alignments'
        iterator = zip(alignments, [r for _, r in scripts.iterrows()])
        examples.extend([
            Example(a, s[text_column], recording_file_path, speaker,
                    {k: v for k, v in s.items() if k != text_column}) for a, s in iterator
        ])

    random.shuffle(examples)
    # NOTE: This assumes that a negligible amount of data is unusable in each example.
    splits = cumulative_split(examples, splits,
                              lambda e: e.alignments[-1][1][1] - e.alignments[0][1][0])

    return tuple(_alignment_generator(s, max_seconds) for s in splits)
