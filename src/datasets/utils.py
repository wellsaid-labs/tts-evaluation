"""
The other idea is... pregenerate a million rows. This might be the easiest approach to deal with
all of this... It then depends on how much you think annotations matter. For context, the model
will go through the data 44 million times or more... So it'll repeat the data 44x.

Let's specify this a bit more...

_gcs_alignment_dataset_loader:
    - Download and load the dataset
    - Sample from the dataset (including splitting and shuffling)
    - TODO: Print the size of each dataset loaded.
    - TODO: Return the size of the dataset alongside the generator.
    - TODO: Handle distributed: Each process does not need download the dataset; however, it's fine
      if they all have their own copy of the data.
      The generator will need to be replicated accross workers in the `DataLoader`.

get_dataset:
    - TODO: Get each of the datasets with dataset loader.
    - TODO: Return a generator that runs non-generalizable (related to hyperparameters and unrelated to input_encoder)
      preprocessing on our datasets.
        - Sampling: The datasets should be sampled equally per speaker; however, we'll eventually
            want to transition away from that model so that we can take advantage of more data.
            We'll need to ensure the number of hours is balanced. We can just sample from
            the dataset with the least number of hours sampled so far...
        - Filtering: (Brainstorm)
          - If `audio_path` is not found
          - If there is no text
          - If there is no audio
          - Remove particular books or speakers
          - Too much audio or too little audio per alignment and it's phonetics.
          - Too many characters or too little characters per alignment.
          - Too quiet?
          - There are numbers
          - Bad alignments
        - Normalize audio w/ ffmpeg and the settings Rhyan sent me.
    - TODO: Using `HParams` we can parameterize the dataset generator function.

- The idea is... we need a function that'll preprocess single rows. It'll also use multiple
  workers to preprocess more rows faster. It'll cache the first 100,000 rows preprocessed.
  It won't start training until the first 100,000 rows are preprocessed. It'll fetch
  older rows of data, if there are no new rows of data to use.
- The concern is that... we cannot effectively cache the `_load_fn` because the `DataLoader`
  has multiple workers running `_load_fn`.
- The solution to that concern is... `_load_fn` still loads a single row. We have a sampler
  that requests 100,000 rows before it produces the first sample. Unfortunately, the samplers
  don't work that way... The samplers largely work with indicies rather than the actual data.
- An additional concern is that... the data loader will also need to replicate the generator,
  which is not feasible. We can't create the generator... at least yet.
- The solution is... `_load_fn` saves examples as they are tabulated under some unique key
  until the filesystem reaches 100,000 examples saved. Since the generator is random, the
  processes can run it indepedently and load data independently. The cache is loaded
  on startup, and each process grabs data based on the worker key.
  Instead of using `_load_fn` which is a "map-style dataset" we can use an "iterable style
  dataset". It better fits the situation.
- Question: How do we handle data transfer?
    - Should each function have it's own copy of the dataset? Likely not.
- Question: How do we handle vocabulary? Since the data is autogenerated... We can't go through
  the entire dataset. We could automatically add additional embeddings (depending on the input).
  We could also seperate a concept of the "data_generator" and the "data". There are similar paterns
  to this with IO. The generator is simply used for reading the data.
- Question: What preprocessing does HParams handle and what preprocessing does `_load_fn` handle?
  I think it depends on the principles of modularity. Which preprocessing is specific to the
  training process? Which is general to the dataset?
- Question: The sampler requires us to be able to grab a specific index of a dataset. Should we
  be using the sampler at all? We can integrate with the samplers a little bit... We can wrap
  an iterable with a mapping interface. Basically, if you request a particular index, then
  the generator will generate up to that index. Also, we'd want to remove that index afterwards
  so that we don't have a memory failure.
- IDEA: Instead of an `IterableDataset` we use a regular dataset. Given a particular index,
  it'll generate up to that index and return it. Any unused data will be stored in a dictionary.
  Everytime we lookup some data, it's deleted afterwards. Or there is a maximum number of indicies
  that can be stored, and it's implemented as a FIFO queue.
- QUESTION: Does the model have to take in tensors only? Can it take in a regular list? Can
  we batch a regular list of tensors? The tensors will then be expanded based on a word mapping
  afterwards...?
    - collate_fn: We can write a custom `collate_fn` in order to handle the batching.
    - `DistributedDataParallel`: Doesn't have any custom logic during the forward pass that'd
      prevent me from passing a list in.
- IDEA: The attention module is already able to operate efficiently with a batch of sequences.
  We could "batch" the word-level, character-level, and phone-level sequences. We could even
  reuse the same encoder between the character-level and phone-level sequences. I'm a bit more
  skeptical about using the word-level sequences with the same encoder since they are so different.
- IDEA: Instead of implementing a caching method for predicted spectrogram output, we could
  add a GAN to the spectrogram. I'd prefer that...
- QUESTION: Can we use the same dataset loader for lj_speech or m-ailabs dataset? No. The dataset
  loader that we built samples longer alignments less often. That said... We could just push
  max_seconds to `infinity` and it'll work on the other type of dataset as well.
- QUESTION: With a float('inf'), that means that longer alignments are sampled more often. Should
  that be the case? That means that longer alignments are sampled more frequently, which is
  against the goal of the function. The goal is that alignments are sampled an equal amount. This
  is messed up because... if the script length is shorter than the length of the span, than the
  sampler script. As the span length gets larger, the number of spans inside of example shrinks.
  Uptill the the point where, there is only one span. If there is only one span, then each script
  is effectively equal. Should we correct for that? Something like... size of the span divided
  by the size of the script. For example if we make this problem discrete it looks like this:
x x x [0, 0, 0, 0] with a span of 3 has 6 spans inside of it. If the span is 4 it has... 7 spans
  inside it. What I am really saying is that it gets less and less likely to only sample 1, 2, 3
  items. The larger it gets the more likely we are always going to sample the entire script. After a span of 4, there
  will always be only 2 options for sampling 1, 2, 3 items. Every other item will be the entire span.
  So at a span of 60, there will be 6 options to sample 1, 2, 3. +1 ~54 ish options for sampling
  the entire script. Do we implicitly rely on the length to be sampler than the script.
  If the likelihood of capturing the entire script goes higher, then we end up missing elements...
  Ok... So what is that likelihood?
  x x x [0, 0, 0]
  span 0.0: 0%
  span 1.0: 0%
  span 2.0: 0%
  span 3.0: ~0%
  span 4.0: 1 / 7 (random.uniform(-4, 3))
  span 5.0: 2 / 8 (random.uniform(-5, 3))
  span 6.0: 3 / 9 (random.uniform(-6, 3))
  span 7.0: 4 / 10 (random.uniform(-7, 3))
  span 60.0: 57 / 63 (random.uniform(-7, 3))
  function(span_length, script_length): (span_length - script_length) / (span_length + script_length)
  This is the probability of times that.... the weight should be equal to 1.0.

  Another way to think about this is as a probability distribution. What is the probability
  distribution implying? I think it's implying there are more spans in one distribution than
  another distribution. If the span is sufficient small, that's true. So what we're really
  trying to say is... we're just counting the number of spans. The larger the span, the more
  everythign looks equal from the prespective of the span. Okay... So the solution is...
  the weight should be the script_length + span_length
- QUESTION: Why not just uniformly pick an alignment? That'll pass all our tests... Is there any
  specific test that it wouldn't pass? (We'll still need to do a length with a random generator...)
  Okay... The test that might be difficult to pass if we sample alignments uniformly is specifically
  around spans. We could then get a startpoint inside it with the same min / max idea. We'll still
  need to pick an end point.
- QUESTION: What should be the probability of picking a particular length to offset the boundary
  issues?
  [0, 0, 0]
  span 0.0: 3/3% (random.uniform(0, 3))
  span 1.0: 2/4 (0 - 2) (random.uniform(-1, 3))
  span 2.0: 1/5 (0 - 1) (random.uniform(-2, 3))
  span 3.0: 0/6 (0 - 1) (random.uniform(-3, 3))
  This is the probability that the length is not correct. So we should increase it by that much,
  the distribution should look like this:
  1.0 4/2
  2.0 5/1
  3.0 (inf)

  Or... (script_length - span_length) / (span_length + script_length)
  (3 - 1) / (3 + 1)
- QUESTION: What's the probability for a certain length to appear?
  [0, 0, 0] (max_seconds=3)

  length 0 - 0.5:
      length 0 - 0.5:
  length 0.5 - 1.5:
      length 0 - 0.5: 8 (25%) 1 / 4 random.uniform(-1, 3)
      length 0.5 - 1.5: 25 (75%) 3 / 4
  length 1.5 - 2.5:
      length 0 - 0.5: 7 (20%) 1 / 5 random.uniform(-2, 3)
      length 0.5 - 1.5: 13 (40%) 2 / 5
      length 1.5 - 2: 13 (40%) 2 / 5
  length 2.5 - 3:
      length 0 - 0.5: 2.9 (17.5%) 1.05 / 6 random.uniform(-3, 3)
      length 0.5 - 1.5: 5.8 (35%) 2.1 / 6
      length 1.5 - 2.5: 5.8 (35%) 2.1 / 6
      length 2.5 - 3: 2.1 (12.5%) 0.75 / 6

  length 1
      random.uniform(-1, 3) length=1: 75% 3/4 [-1 0 1 2 3] ()
      random.uniform(-2, 3) length=2: 40% 2/5 [-2 -1 0 1 2 3] (0.5 - 2,1.5 - 2) (1.5, 2.5)
      random.uniform(-3, 3) length=3: 33% 2/6 [-3 -2 -1 0 1 2 3] (0.5 - 3,1.5 - 3) (1.5, 2.5)
  length 2
      random.uniform(-1, 3) length=1: 0%
      random.uniform(-2, 3) length=2: 33.3%
      random.uniform(-3, 3) length=3: 25% [-3 -2 -1 0 1 2 3] (-1.5,1.5) to (-0.5, 2.5) 0.5 to 1.5
  length 3
      random.uniform(-1, 3) length=1: 0%
      random.uniform(-2, 3) length=2: 0%
      random.uniform(-3, 3) length=3: 25%



Trainer.__init__ or Model.__init__:
  - Iterate through the text, and create a vocab.
  - Cache the graphemes and spaCy, iterate, and create a vocab.
  - Iterate through the speakers, and create a vocab.
  - Create an embedding table for phonemes.
  - Create an embedding table for capitalization

data_loader#_load_fn:
  - TODO: The `DataLoader` instead of taking a `Dataset` and a `Sampler`. It'll take on
    an `IterableDataset`:
        - 1. It's given data (alignments / scripts / audio_path) and a generator function.
        (NOTE: We don't need this... except maybe for signal model training...)
        - 2. It preloads any existing data. (With respect to it's worker id?)
        - 3. Similar to worker, it starts a thread that runs the generator with a queue.
        - 4. Every iteration, if there are items in the queue it yeilds them; otherwise, it'll
             yeild a pre-existing item.
  - TODO: Implement a function that turns turns a sampler into an iterator. The sampler is able
    accept an iterator, and it returns an iterator. The function has a "sample with replacement"
    and "sample without replacement" mode. Use this iterator with the remaining samplers.
  - TODO: Get rid of the Oom sampler because it requires us to iterate through the entire dataset.
    We could try to iterate through a subset of the dataset, in order to prevent an OOM. Let's
    not worry about it for right now.
  - TODO: Get rid of DistributedBatchSampler because the data is randomly generated on each
    process; therefore, we don't need to segment it.
  - TODO: We can get rid of DeterministicSampler because the process will already be
    non-determinsitic.
  - TODO: Get rid of the balanced sampler, since the generator is already balanced by definition.
    Double check this...
  - TODO: Implement additional preprocessing:
    (For spectrogram performance, we should be okay. For signal model, we'll need a solution for
    predicted spectrogram. Ideally, we can just get rid of the need for a predicted spectrogram...
    by adding a GAN module to the spectrogram model.)
    - Preprocess with spaCy (and cache) (36ms) (Cache)
    - Average and rounded (to prevent overfitting) loudness (with ITU-R BS.1770-4) (17ms)
    - Average and rounded (to prevent overfitting) speed (seconds per phoneme).
    - Average and rounded (to prevent overfitting) pitch (with CREPE or SPICE or torchaudio) (671ms) (Don't Implement)
    - For a random number of transitions, compute the (rounded to prevent overfitting) pause
      time in seconds.
    - Lower case the text, and provide an extra feature with regard to capitalization. (in the input encoder?)
    - For a random number of words, provide the phonetic spelling. (522ms) (Cache)
    - Extract and provide any related metadata with regards to book or article.
    - Get spectrogram (38ms)
  - TODO: Following preprocessing, the process should return `SpectrogramModelTrainingRow` with:
      - loudness: torch.FloatTensor [num_tokens] (0 is maximum, -100 is minimum)
      - loudness_mask
      - speed: torch.FloatTensor [num_tokens] (0 is minmimum, 16 characters on average)
      - speed_mask
      - pausing: torch.FloatTensor [num_tokens] (0 is minimum, 1 or 2 seconds)
      - pausing_mask
      - capitalization: torch.LongTensor [num_tokens] (capitalized, not capitalized, not letter)
      - text: torch.LongTensor [num_tokens]
      - word_to_text: list of text spans
      - phoneme: list of phonetic spellings
      - word_vectors: torch.FloatTensor [num_words, word_vector_size]
      - contextual_word_vectors: torch.FloatTensor [num_words, word_vector_size]
      - speaker: torch.LongTensor [1]
      - spectrogram: torch.FloatTensor [num_frames, frame_channels]
      - spectrogram_mask: torch.FloatTensor [num_frames]
      - spectrogram_extended_mask: torch.FloatTensor [num_frames, frame_channels]
      - stop_token: torch.FloatTensor [num_frames]

Trainer#_do_loss_and_maybe_backwards:
  - TODO: Remove `expected_average_spectrogram_length` and replace it with a constant or calculate
    it from the first 100,000k or so examples.

Model#forward:
  - Accept loudness, speed, pausing, capitalization, text, word_to_text, phoneme, word_vectors,
    contextual_word_vectors, speaker
  - Linear layer for loudness, speed, pausing that are applied, if a value is present.
  - Capitalization embedding
  - Embedding for text
  - Embedding for phoneme
  - Learned padding vector between words. It's distinct from the other types of padding that
    we ignore.
  - Map the words vectors and phonemes to the text via word_to_text. Use PyTorch to interpolate,
    so they are the same size. Potentially... before interpolation, we could run the convolutions.
  - Loudness / speed / pausing should default to 0 with an appropriate loudness mask.

Worker:
  - Run spaCy on input for tokenization and for word_to_text vector.
  - Input encoder:
      - Grapheme to phoneme
      - Capitalization
      - Phonemes
  - Accept XML to create loudness, speed, pausing and phoneme.


TODO:
1. Print the size of each dataset loaded.
2. Ensure old datasets like LJ and M-AILABS are converted to the new format.
3. Weigh the datasets based
4. Preprocess Examples:
    1. Add

datasets/
    Within this module
"""
from collections import namedtuple
from enum import Enum
from functools import lru_cache
from math import ceil
from math import floor
from pathlib import Path

import json
import logging
import pprint
import random
import subprocess

from third_party import LazyLoader

import torch
librosa = LazyLoader('librosa', globals(), 'librosa')
pandas = LazyLoader('pandas', globals(), 'pandas')

from src.environment import DATA_PATH
from src.utils import flatten
from src.utils import natural_keys

logger = logging.getLogger(__name__)
pprint = pprint.PrettyPrinter(indent=4)

# Args:
#     alignments (list of tuple(tuple(int, int), tuple(int, int))): List of alignments from
#         `script` to `audio_path`.
#     script (str): The script read in the voice over.
#     audio_path (pathlib.Path): The voice over.
#     speaker (Speaker): Speaker represented in voice over.
#     metadata (dict): Additional metadata associated with this example.
Example = namedtuple(
    'Example', ['alignments', 'script', 'audio_path', 'speaker', 'metadata'],
    defaults=(None, None, None, None, {}))


class Gender(Enum):
    FEMALE = 0
    MALE = 1


class Speaker(object):

    def __init__(self, name, gender):
        self.name = name
        self.gender = gender

    def __eq__(self, other):
        if isinstance(other, Speaker):
            return self.name == other.name and self.gender == other.gender

        # Learn more:
        # https://stackoverflow.com/questions/878943/why-return-notimplemented-instead-of-raising-notimplementederror
        return NotImplemented

    def __hash__(self):
        # Learn more:
        # https://computinglife.wordpress.com/2008/11/20/why-do-hash-functions-use-prime-numbers/
        return 32 * hash(self.name) + 97 * hash(self.gender)

    def __repr__(self):
        return '%s(name=\'%s\', gender=%s)' % (self.__class__.__name__, self.name, self.gender.name)


def dataset_generator(data, max_seconds):
    """ Generate `Example`(s) that are at most `max_seconds` long.

    NOTE:
    - Every alignment has an equal chance of getting sampled, assuming there are no overlaps.
    - Larger slices of alignments are less likely to be sampled, for the most part. See more here:
      https://stats.stackexchange.com/questions/484329/how-do-you-uniformly-sample-spans-from-a-bounded-line/484332#484332

    TODO: Resolve the above bias.
    TODO: Visualize the sampled distribution, in order to ensure it it's reasonable for training.

    Args:
        data (list of Example): List of examples to sample from.
        max_seconds (float): The maximum interval length.

    Returns:
        (iterator of Example)
    """
    assert max_seconds > 0, 'The maximum interval length must be a positive number.'
    if len(data) == 0:
        return iter([])
    if max_seconds == float('inf'):
        while True:
            return random.choice(data)

    min_ = lambda e: e.alignments[0][1][0]
    max_ = lambda e: e.alignments[-1][1][1]
    offset = lambda e: floor(min_(e))

    lookup = [[[] for _ in range(ceil(max_(e)) - offset(e) + 1)] for e in data]
    for i, example in enumerate(data):
        for j, alignment in enumerate(example.alignments):
            for k in range(int(floor(alignment[1][0])), int(ceil(alignment[1][1])) + 1):
                lookup[i][k - offset(example)].append(j)

    weights = torch.FloatTensor([max_(e) - min_(e) for e in data])
    while True:
        length = random.uniform(0, max_seconds)
        # NOTE: The `weight` is based on `start` (i.e. the number of spans)
        index = torch.multinomial(weights + length, 1).item()
        example = data[index]
        start = random.uniform(min_(example) - length, max_(example))
        end = min(start + length, max_(example))
        start = max(start, min_(example))
        part = flatten(lookup[index][int(start) - offset(example):int(end) - offset(example) + 1])
        get = lambda i: example.alignments[i][1]
        overlap = lambda i: (min(end, get(i)[1]) - max(start, get(i)[0])) / (get(i)[1] - get(i)[0])
        random_ = lru_cache(maxsize=None)(lambda i: random.random())
        bounds = (
            next((i for i in part if overlap(i) >= random_(i)), None),
            next((i for i in reversed(part) if overlap(i) >= random_(i)), None),
        )
        if (bounds[0] is not None and bounds[1] is not None and bounds[0] <= bounds[1] and
                get(bounds[1])[1] - get(bounds[0])[0] > 0 and
                get(bounds[1])[1] - get(bounds[0])[0] <= max_seconds):
            yield example._replace(alignments=example.alignments[bounds[0]:bounds[1] + 1])


def dataset_loader(root_directory_name,
                   speaker,
                   directory=DATA_PATH,
                   gcs_path='gs://wellsaid_labs_datasets/hilary_noriega',
                   alignments_directory_name='alignments',
                   recordings_directory_name='recordings',
                   scripts_directory_name='scripts',
                   text_column='Content',
                   max_seconds=15):
    """ Load an alignment text-to-speech (TTS) dataset from GCS.

    TODO: Print dataset size with `seconds_to_string`.

    The structure of the dataset should be:
        - The file structure is similar to:
            {gcs_path}/
            ├── {alignments_directory_name}/  # Alignments between recordings and scripts
            │   ├── audio1.json
            │   └── ...
            ├── {recordings_directory_name}/  # Voice overs
            │   ├── audio1.wav                # NOTE: Most audio file formats are accepted.
            │   └── ...
            └── {scripts_directory_name}/     # Voice over scripts with related metadata
                ├── audio1-script.csv
                └── ...
        - The alignments, recordings, and scripts directory should contain the same number of
          similarly named files.
        - The dataset contain data representing only one speaker.

    Args:
        root_directory_name (str): Name of the directory inside `directory` to store data.
        speaker (src.datasets.Speaker): The speaker represented by this dataset.
        directory (str or Path, optional): Directory to cache the dataset.
        gcs_path (str, optional): The base GCS path storing the data.
        alignments_gcs_path (str, optional): The name of the alignments directory on GCS.
        recordings_gcs_path (str, optional): The name of the voice over directory on GCS.
        scripts_gcs_path (str, optional): The name of the voice over script directory on GCS.
        text_column (str, optional): The voice over script column in the CSV script files.
        max_seconds (int, optional): The length of an example.

    Returns:
        (tuple of generator of Example): A generator of `Example`s is returned for each split.
    """
    logger.info('Loading `%s` speech dataset', root_directory_name)

    root = (Path(directory) / root_directory_name).absolute()
    root.mkdir(exist_ok=True)
    directories = [alignments_directory_name, recordings_directory_name, scripts_directory_name]
    directories = [root / d for d in directories]
    for directory, suffix in zip(directories, ('.json', '', '.csv')):
        directory.mkdir(exist_ok=True)
        command = 'gsutil -m cp -n %s/%s/*%s %s/' % (gcs_path, directory.name, suffix, directory)
        subprocess.run(command.split(), check=True)

    files = (sorted(d.iterdir(), key=lambda p: natural_keys(p.name)) for d in directories)
    examples = []
    for alignment_file_path, recording_file_path, script_file_path in zip(*tuple(files)):
        alignments = json.loads(alignment_file_path.read_text())
        scripts = pandas.read_csv(str(script_file_path.absolute()))
        assert len(scripts) == len(alignments), 'Expected equal number of scripts and alignments'
        iterator = zip(alignments, [r for _, r in scripts.iterrows()])
        examples.extend([
            Example(a, s[text_column], recording_file_path, speaker,
                    {k: v for k, v in s.items() if k != text_column}) for a, s in iterator
        ])

    return examples
